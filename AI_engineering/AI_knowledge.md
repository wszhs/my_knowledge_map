# AI Knowledge

## Material
- [机器学习准则（期望风险、经验风险、结构风险)](https://zhuanlan.zhihu.com/p/159189617)
- [从变分编码、信息瓶颈到正态分布：论遗忘的重要性](https://mp.weixin.qq.com/s/SGTp7PoknF64wcuW2qdxhQ)
- [殊途同归的零阶优化和策略梯度](https://kexue.fm/archives/7737#mjx-eqn-eq%3Abase)
- [从动力学角度看优化算法（七）：SGD ≈ SVM？](https://kexue.fm/archives/8009)
- [漫谈重参数：从正态分布到Gumbel Softmax](https://kexue.fm/archives/6705)
- [积分梯度：一种新颖的神经网络可视化方法](https://kexue.fm/archives/7533)
- [梯度下降和EM算法：系出同源，一脉相承](https://kexue.fm/archives/4277)
- [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765)
- [从动力学角度看优化算法：GAN的第三个阶段](https://zhuanlan.zhihu.com/p/65953336)
- [从Boosting学习到神经网络：看山是山？](https://kexue.fm/archives/3873)

## 学习资料
- 李沫读论文 [github](https://github.com/mli/paper-reading) [b站](https://www.bilibili.com/video/BV1iT4y1d7zP?spm_id_from=333.337.search-card.all.click) [动手学机器学习](https://zh-v2.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html#subsec-cnn-rnn-self-attention)